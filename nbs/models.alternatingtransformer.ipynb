{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6420ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.alternatingtransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5180a",
   "metadata": {},
   "source": [
    "# Alternating Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04fb10d",
   "metadata": {},
   "source": [
    "Vanilla Transformer with alternating spatial and temporal attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3948dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuralforecast.common._modules import (\n",
    "    TransEncoderLayer, TransEncoder,\n",
    "    TransDecoderLayer, TransDecoder,\n",
    "    DataEmbedding, AttentionLayer, FullAttention,\n",
    ")\n",
    "from neuralforecast.models.vanillatransformer import VanillaTransformer\n",
    "from neuralforecast.common._base_model import BaseModel\n",
    "from neuralforecast.losses.pytorch import MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import logging\n",
    "import warnings\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import show_doc\n",
    "from neuralforecast.common._model_checks import check_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45077851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _AltEncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder layer with spatial or temporal attention.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, hidden_size, n_head, conv_hidden_size, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.layer = TransEncoderLayer(\n",
    "            AttentionLayer(\n",
    "                FullAttention(mask_flag=False, attention_dropout=dropout, output_attention=False),\n",
    "                hidden_size,\n",
    "                n_head,\n",
    "            ),\n",
    "            hidden_size,\n",
    "            conv_hidden_size,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, N, D]\n",
    "        B, L, N, D = x.shape\n",
    "        if self.mode == 'temporal':\n",
    "            x_r = x.reshape(B * L, N, D)\n",
    "            out, _ = self.layer(x_r, attn_mask=None)\n",
    "            x = out.reshape(B, L, N, D)\n",
    "        else:  # spatial\n",
    "            x_r = x.permute(0, 2, 1, 3).reshape(B * N, L, D)\n",
    "            out, _ = self.layer(x_r, attn_mask=None)\n",
    "            x = out.reshape(B, N, L, D).permute(0, 2, 1, 3)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _AltEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, n_head, conv_hidden_size, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            _AltEncoderLayer('temporal' if i % 2 == 0 else 'spatial', hidden_size, n_head, conv_hidden_size, dropout, activation)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x), None\n",
    "\n",
    "\n",
    "class _AltDecoderLayer(nn.Module):\n",
    "    def __init__(self, mode, hidden_size, n_head, conv_hidden_size, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.layer = TransDecoderLayer(\n",
    "            AttentionLayer(\n",
    "                FullAttention(mask_flag=True, attention_dropout=dropout, output_attention=False),\n",
    "                hidden_size,\n",
    "                n_head,\n",
    "            ),\n",
    "            AttentionLayer(\n",
    "                FullAttention(mask_flag=False, attention_dropout=dropout, output_attention=False),\n",
    "                hidden_size,\n",
    "                n_head,\n",
    "            ),\n",
    "            hidden_size,\n",
    "            conv_hidden_size,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        B, L, N, D = x.shape\n",
    "        enc_len = cross.shape[1]\n",
    "        if self.mode == 'temporal':\n",
    "            x_r = x.reshape(B * L, N, D)\n",
    "            cross_r = cross.reshape(B * enc_len, N, D)\n",
    "            out = self.layer(x_r, cross_r, x_mask=x_mask, cross_mask=cross_mask)\n",
    "            x = out.reshape(B, L, N, D)\n",
    "        else:\n",
    "            x_r = x.permute(0, 2, 1, 3).reshape(B * N, L, D)\n",
    "            cross_r = cross.permute(0, 2, 1, 3).reshape(B * N, enc_len, D)\n",
    "            out = self.layer(x_r, cross_r, x_mask=x_mask, cross_mask=cross_mask)\n",
    "            x = out.reshape(B, N, L, D).permute(0, 2, 1, 3)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _AltDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, n_head, conv_hidden_size, dropout, activation, projection):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            _AltDecoderLayer('temporal' if i % 2 == 0 else 'spatial', hidden_size, n_head, conv_hidden_size, dropout, activation)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "class AlternatingTransformer(VanillaTransformer):\n",
    "    \"\"\"Vanilla Transformer with alternating spatial and temporal attention.\"\"\"\n",
    "\n",
    "    EXOGENOUS_FUTR = True\n",
    "    EXOGENOUS_HIST = False\n",
    "    EXOGENOUS_STAT = False\n",
    "    MULTIVARIATE = True\n",
    "    RECURRENT = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        h: int,\n",
    "        input_size: int,\n",
    "        n_series: int,\n",
    "        stat_exog_list=None,\n",
    "        hist_exog_list=None,\n",
    "        futr_exog_list=None,\n",
    "        exclude_insample_y: bool = False,\n",
    "        decoder_input_size_multiplier: float = 0.5,\n",
    "        hidden_size: int = 128,\n",
    "        dropout: float = 0.05,\n",
    "        n_head: int = 4,\n",
    "        conv_hidden_size: int = 32,\n",
    "        activation: str = \"gelu\",\n",
    "        encoder_layers: int = 2,\n",
    "        decoder_layers: int = 1,\n",
    "        loss=MAE(),\n",
    "        valid_loss=None,\n",
    "        max_steps: int = 5000,\n",
    "        learning_rate: float = 1e-4,\n",
    "        num_lr_decays: int = -1,\n",
    "        early_stop_patience_steps: int = -1,\n",
    "        val_check_steps: int = 100,\n",
    "        batch_size: int = 32,\n",
    "        valid_batch_size: Optional[int] = None,\n",
    "        windows_batch_size=1024,\n",
    "        inference_windows_batch_size: int = 1024,\n",
    "        start_padding_enabled=False,\n",
    "        step_size: int = 1,\n",
    "        scaler_type: str = \"identity\",\n",
    "        random_seed: int = 1,\n",
    "        drop_last_loader: bool = False,\n",
    "        alias: Optional[str] = None,\n",
    "        optimizer=None,\n",
    "        optimizer_kwargs=None,\n",
    "        lr_scheduler=None,\n",
    "        lr_scheduler_kwargs=None,\n",
    "        dataloader_kwargs=None,\n",
    "        **trainer_kwargs,\n",
    "    ):\n",
    "        BaseModel.__init__(\n",
    "            self,\n",
    "            h=h,\n",
    "            input_size=input_size,\n",
    "            n_series=n_series,\n",
    "            stat_exog_list=stat_exog_list,\n",
    "            hist_exog_list=hist_exog_list,\n",
    "            futr_exog_list=futr_exog_list,\n",
    "            exclude_insample_y=exclude_insample_y,\n",
    "            loss=loss,\n",
    "            valid_loss=valid_loss,\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            num_lr_decays=num_lr_decays,\n",
    "            early_stop_patience_steps=early_stop_patience_steps,\n",
    "            val_check_steps=val_check_steps,\n",
    "            batch_size=batch_size,\n",
    "            valid_batch_size=valid_batch_size,\n",
    "            windows_batch_size=windows_batch_size,\n",
    "            inference_windows_batch_size=inference_windows_batch_size,\n",
    "            start_padding_enabled=start_padding_enabled,\n",
    "            step_size=step_size,\n",
    "            scaler_type=scaler_type,\n",
    "            drop_last_loader=drop_last_loader,\n",
    "            alias=alias,\n",
    "            random_seed=random_seed,\n",
    "            optimizer=optimizer,\n",
    "            optimizer_kwargs=optimizer_kwargs,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "            dataloader_kwargs=dataloader_kwargs,\n",
    "            **trainer_kwargs,\n",
    "        )\n",
    "\n",
    "        self.label_len = int(np.ceil(input_size * decoder_input_size_multiplier))\n",
    "        if (self.label_len >= input_size) or (self.label_len <= 0):\n",
    "            raise Exception(\n",
    "                f\"Check decoder_input_size_multiplier={decoder_input_size_multiplier}, range (0,1)\"\n",
    "            )\n",
    "\n",
    "        if activation not in [\"relu\", \"gelu\"]:\n",
    "            raise Exception(f\"Check activation={activation}\")\n",
    "\n",
    "        self.c_out = self.loss.outputsize_multiplier\n",
    "        self.enc_in = 1\n",
    "        self.dec_in = 1\n",
    "        self.n_series = n_series\n",
    "\n",
    "        self.enc_embedding = DataEmbedding(\n",
    "            c_in=self.enc_in,\n",
    "            exog_input_size=self.futr_exog_size,\n",
    "            hidden_size=hidden_size,\n",
    "            pos_embedding=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.dec_embedding = DataEmbedding(\n",
    "            self.dec_in,\n",
    "            exog_input_size=self.futr_exog_size,\n",
    "            hidden_size=hidden_size,\n",
    "            pos_embedding=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.encoder = _AltEncoder(\n",
    "            num_layers=encoder_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            n_head=n_head,\n",
    "            conv_hidden_size=conv_hidden_size,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        )\n",
    "        self.decoder = _AltDecoder(\n",
    "            num_layers=decoder_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            n_head=n_head,\n",
    "            conv_hidden_size=conv_hidden_size,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            projection=nn.Linear(hidden_size, self.c_out, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, windows_batch):\n",
    "        insample_y = windows_batch[\"insample_y\"]  # [B, L, N]\n",
    "        futr_exog = windows_batch[\"futr_exog\"]  # [B, F, L+h, N]\n",
    "        B, L, N = insample_y.shape\n",
    "\n",
    "        insample_y = insample_y.permute(0, 2, 1).reshape(B * N, L, 1)\n",
    "        if self.futr_exog_size > 0:\n",
    "            futr = futr_exog.permute(0, 3, 2, 1).reshape(B * N, L + self.h, self.futr_exog_size)\n",
    "            x_mark_enc = futr[:, : L, :]\n",
    "            x_mark_dec = futr[:, -(self.label_len + self.h) :, :]\n",
    "        else:\n",
    "            x_mark_enc = None\n",
    "            x_mark_dec = None\n",
    "\n",
    "        x_dec = torch.zeros(size=(B * N, self.h, 1), device=insample_y.device)\n",
    "        x_dec = torch.cat([insample_y[:, -self.label_len :, :], x_dec], dim=1)\n",
    "\n",
    "        enc_out = self.enc_embedding(insample_y, x_mark_enc)\n",
    "        enc_out = enc_out.view(B, N, L, -1).permute(0, 2, 1, 3)\n",
    "        enc_out, _ = self.encoder(enc_out)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = dec_out.view(B, N, self.label_len + self.h, -1).permute(0, 2, 1, 3)\n",
    "        dec_out = self.decoder(dec_out, enc_out)\n",
    "\n",
    "        forecast = dec_out[:, -self.h :, :, :]\n",
    "        forecast = forecast.reshape(B, self.h, N * self.c_out)\n",
    "        return forecast"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
